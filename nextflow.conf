cascaded=true
classifier_first=true # if false pipeline starts with extractor if True the first component of the pipeline will be classifier
gpu_classifier=0,1,2,3,4,5,6,7
gpu_number_tsc=2,3
gpu_number_psc=4,5
gpu_number_osc=6,7
gpu_number_protest=0,1
gpu_token=0,1,2,3,4,5,6
gpu_number_place=7 # This model does not work with multi gpu. Also this is the only model with cpu option in this pipeline (This option is very slow).
input="/PATH_TO_INPUT_FOLDER/" #with backslash at the end
output="/PATH_TO_OUTPUT_FOLDER/" #with backslash at the end
resume=false
files_start_with="http*" #file name pattern, must end with json if classifier_first is TRUE
source_lang="English"
source=3
doc_batchsize=40
token_batchsize=48 #8
prefix="/home/username/PATH_TO_REPO" # no backslash at the end
extractor_script_path=$prefix"/bin/extract/peoples_chaina.py" # path to html to text script
#out_to_csv script's parameters
out_output_type="csv" #type of output
out_name_output_file="outfile" # name of outfile
out_date_key="time" # the key of date in the dataset , =time in thehindu dataset!  ##dont leave space between variable name and equal mark and its value
filter_unprotested_sentence=False
filter_unprotested_documents=False
#TODO : add docs to each parameter
#add python path parameter
sent_batchsize=64
RUN_DOC=true # Whether to run doc level -> Input files must contain "text" and optionally "id"
RUN_SENT=true # Whether to run sent level -> If run_doc is false, then your input files must contain "sentences"
RUN_TOK=true # Whether to run tok level -> If run_doc is false, then your input files must contain ["sentences", "id"]
RUN_POST=false # Whether to run post processing stuff
